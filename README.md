# DHAI Survival Analysis: TexGISa

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

**Time-dependent EXtreme Gradient Integration for Survival Analysis (TexGISa)** - An interpretable deep learning framework for survival analysis with expert knowledge integration.

## Table of Contents

- [Overview](#overview)
- [Key Features](#key-features)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Core Capabilities](#core-capabilities)
- [Usage Examples](#usage-examples)
- [API Documentation](#api-documentation)
- [Performance](#performance)
- [Contributing](#contributing)
- [Citation](#citation)
- [License](#license)

## Overview

TexGISa is a novel deep learning framework for survival analysis that combines the predictive power of neural networks with interpretability through **Time-dependent EXtreme Gradient Integration (TEXGI)**. Unlike traditional black-box deep learning models, TexGISa provides:

- **Time-dependent feature importance** via Expected Gradients
- **Expert knowledge integration** during model training
- **Interpretable predictions** with attribution explanations
- **Discrete-time hazard modeling** for flexible survival prediction

This implementation is based on our research published at **ICDM 2024**: *"TexGISa: Interpretable and Interactive Deep Survival Analysis with Time-dependent Extreme Gradient Integration"*.

### What Makes TexGISa Different?

**Traditional Survival Models:**
- Cox PH: Linear assumptions, limited flexibility
- Random Survival Forests: Black-box, no feature importance over time
- Standard Neural Networks: Lack interpretability

**TexGISa:**
- âœ… Deep learning flexibility without black-box limitations
- âœ… Time-dependent feature importance using TEXGI
- âœ… Expert knowledge constraints during training
- âœ… Full survival curve prediction with explanations

## Key Features

### ðŸ§  Interpretable Deep Learning
- **TEXGI (Time-dependent EXtreme Gradient Integration)**: Compute feature importance using Expected Gradients
- **Time-dependent attributions**: Understand how feature importance changes over time
- **Attribution visualization**: Built-in tools for explaining predictions

### ðŸ‘¨â€âš•ï¸ Expert Knowledge Integration
- **Expert rules**: Incorporate domain knowledge as constraints
- **Rule-based training**: Penalize model when attributions contradict expert knowledge
- **Flexible rule specification**: Support for feature conditions and expected effects

### ðŸŽ¯ Flexible Survival Modeling
- **Discrete-time hazards**: Model hazard at each time interval
- **Multi-task learning**: Joint prediction across time bins
- **Temporal smoothness**: Optional regularization for smooth hazard curves

### âš¡ Production Ready
- **GPU acceleration**: Full PyTorch implementation
- **Scikit-learn compatible**: Familiar `.fit()` and `.predict()` API
- **Type hints**: Complete type annotations throughout
- **Comprehensive testing**: Extensive test suite

## Installation

### From PyPI (Recommended)

```bash
pip install texgisa-survival
```

### From Source

```bash
git clone https://github.com/dhai-lab/texgisa_survival.git
cd texgisa_survival
pip install -e .
```

### Development Installation

```bash
git clone https://github.com/dhai-lab/texgisa_survival.git
cd texgisa_survival
pip install -e ".[dev]"
```

### Requirements

- Python 3.8 or higher
- PyTorch 1.9.0 or higher
- NumPy, Pandas, scikit-learn
- See `pyproject.toml` for complete list

## Quick Start

```python
import numpy as np
from texgisa_survival import TexGISa

# Prepare your data
X = np.random.randn(1000, 10)  # Features
time = np.random.exponential(100, 1000)  # Time to event
event = np.random.binomial(1, 0.7, 1000)  # Event indicator (1=event, 0=censored)

# Create and train model
model = TexGISa(
    hidden_layers=[64, 32],
    num_time_bins=10,
    dropout=0.1,
    lambda_expert=0.1,  # Weight for expert knowledge
    ig_steps=20,  # Integration steps for TEXGI
    random_state=42
)

# Optional: Add expert knowledge
model.add_expert_rule(
    feature='age',  # Feature name or index
    relation='>',  # Condition: '>', '>=', '<', '<=', '=='
    threshold='mean',  # Threshold: numeric value or 'mean', 'median'
    sign=1,  # Expected effect: 1 (positive) or -1 (negative)
    weight=1.0  # Rule importance weight
)

# Train the model
model.fit(X, time, event, epochs=100, verbose=1)

# Make predictions
risk_scores = model.predict_risk(X)
survival_probs = model.predict_survival(X)

# Get interpretable feature importance
importance = model.get_feature_importance(method='texgi')
print("Feature importance:", importance['importance'])
print("Uncertainty (std):", importance['std'])
```

## Core Capabilities

### 1. Survival Prediction

```python
# Predict risk scores (higher = higher risk)
risk = model.predict_risk(X_test)

# Predict survival probabilities at all time bins
survival_curves = model.predict_survival(X_test)

# Predict survival at specific times
survival_at_times = model.predict_survival(X_test, times=[30, 60, 90])
```

### 2. Feature Importance (TEXGI)

```python
# Compute TEXGI feature importance
importance_result = model.get_feature_importance(method='texgi')

# Get importance scores (normalized to sum to 1)
importance_scores = importance_result['importance']

# Get uncertainty estimates
importance_std = importance_result['std']

# Importance scores represent average attribution across all time bins
# Higher values = more important features
```

### 3. Expert Knowledge Integration

```python
# Example: Age has positive effect on risk when above average
model.add_expert_rule(
    feature='age',
    relation='>',
    threshold='mean',
    sign=1,  # Positive effect
    weight=1.0
)

# Example: Treatment has negative effect on risk
model.add_expert_rule(
    feature='treatment',
    relation='==',
    threshold=1,
    sign=-1,  # Negative effect (protective)
    weight=2.0  # Higher weight = stronger constraint
)

# Train with expert rules
model.fit(X, time, event, epochs=100)
```

### 4. Model Configuration

```python
model = TexGISa(
    # Architecture
    hidden_layers=[128, 64, 32],  # Network architecture
    num_time_bins=20,  # Number of discrete time intervals
    dropout=0.2,  # Dropout rate
    activation='relu',  # Activation function

    # Regularization
    lambda_smooth=0.01,  # Temporal smoothness weight
    lambda_expert=0.1,  # Expert knowledge weight

    # TEXGI configuration
    ig_steps=50,  # Integration steps for Expected Gradients

    # Training
    device='cuda',  # 'cpu', 'cuda', or 'mps'
    random_state=42
)
```

## Usage Examples

### Example 1: Basic Survival Analysis

```python
from texgisa_survival import TexGISa
from sklearn.model_selection import train_test_split
import numpy as np

# Load data
X, time, event = load_your_data()

# Split data
X_train, X_test, time_train, time_test, event_train, event_test = train_test_split(
    X, time, event, test_size=0.2, random_state=42
)

# Train model
model = TexGISa(hidden_layers=[64, 32], num_time_bins=10)
model.fit(X_train, time_train, event_train, epochs=100)

# Evaluate
from texgisa_survival.metrics import concordance_index
c_index = concordance_index(time_test, model.predict_risk(X_test), event_test)
print(f"C-index: {c_index:.3f}")
```

### Example 2: With Expert Knowledge

```python
# Create model with expert knowledge
model = TexGISa(
    hidden_layers=[64, 32],
    num_time_bins=10,
    lambda_expert=0.2  # Enable expert loss
)

# Add domain knowledge rules
model.add_expert_rule('tumor_size', '>', 'median', sign=1, weight=1.5)
model.add_expert_rule('treatment_type', '==', 1, sign=-1, weight=1.0)
model.add_expert_rule('patient_age', '>', 65, sign=1, weight=0.8)

# Train with rules
model.fit(X_train, time_train, event_train, epochs=100)

# Model will learn to respect these rules during training
```

### Example 3: Feature Importance Analysis

```python
# Train model
model = TexGISa(hidden_layers=[64, 32])
model.fit(X_train, time_train, event_train, epochs=100)

# Get TEXGI importance
importance_dict = model.get_feature_importance(method='texgi')

# Visualize top features
import pandas as pd
import matplotlib.pyplot as plt

feature_names = ['age', 'tumor_size', 'grade', ...]  # Your feature names
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': importance_dict['importance'],
    'std': importance_dict['std']
})

# Sort by importance
importance_df = importance_df.sort_values('importance', ascending=False)

# Plot
plt.figure(figsize=(10, 6))
plt.barh(importance_df['feature'][:10], importance_df['importance'][:10])
plt.xlabel('TEXGI Feature Importance')
plt.title('Top 10 Most Important Features')
plt.tight_layout()
plt.show()
```

### Example 4: Cross-Validation

```python
from sklearn.model_selection import KFold
import numpy as np

# Prepare cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
c_indices = []

for train_idx, val_idx in kf.split(X):
    X_train, X_val = X[train_idx], X[val_idx]
    time_train, time_val = time[train_idx], time[val_idx]
    event_train, event_val = event[train_idx], event[val_idx]

    # Train model
    model = TexGISa(hidden_layers=[64, 32], random_state=42)
    model.fit(X_train, time_train, event_train, epochs=50, verbose=0)

    # Evaluate
    risk_val = model.predict_risk(X_val)
    c_idx = concordance_index(time_val, risk_val, event_val)
    c_indices.append(c_idx)

print(f"Mean C-index: {np.mean(c_indices):.3f} Â± {np.std(c_indices):.3f}")
```

## API Documentation

### TexGISa Class

**Main class for Time-dependent EXtreme Gradient Integration Survival Analysis.**

#### Constructor Parameters

```python
TexGISa(
    hidden_layers: List[int] = [128, 64, 32],
    num_time_bins: int = 20,
    dropout: float = 0.1,
    activation: str = 'relu',
    lambda_smooth: float = 0.0,
    lambda_expert: float = 0.0,
    ig_steps: int = 20,
    device: str = 'cpu',
    random_state: Optional[int] = None,
    verbose: int = 1
)
```

**Parameters:**
- `hidden_layers`: Hidden layer dimensions for neural network
- `num_time_bins`: Number of discrete time intervals for hazard prediction
- `dropout`: Dropout probability for regularization
- `activation`: Activation function ('relu', 'elu', 'selu')
- `lambda_smooth`: Weight for temporal smoothness regularization
- `lambda_expert`: Weight for expert knowledge constraint loss
- `ig_steps`: Number of integration steps for TEXGI (more = more accurate)
- `device`: Computing device ('cpu', 'cuda', 'mps')
- `random_state`: Random seed for reproducibility
- `verbose`: Verbosity level (0=silent, 1=progress)

#### Methods

**`fit(X, time, event, **kwargs)`**

Train the TexGISa model.

```python
model.fit(
    X,  # Feature matrix [n_samples, n_features]
    time,  # Survival times [n_samples]
    event,  # Event indicators [n_samples]
    validation_data=None,  # Optional (X_val, time_val, event_val)
    epochs=100,
    batch_size=32,
    learning_rate=0.001,
    early_stopping=True,
    patience=10,
    verbose=1
)
```

**`predict_risk(X)`**

Predict risk scores (higher = higher risk).

```python
risk_scores = model.predict_risk(X_test)  # Returns [n_samples]
```

**`predict_survival(X, times=None)`**

Predict survival probabilities.

```python
# All time bins
survival = model.predict_survival(X_test)  # Returns [n_samples, n_time_bins]

# Specific times
survival = model.predict_survival(X_test, times=[30, 60, 90])  # Returns [n_samples, 3]
```

**`get_feature_importance(method='texgi', n_steps=None)`**

Compute feature importance.

```python
importance = model.get_feature_importance(method='texgi')
# Returns: {'importance': array([...]), 'std': array([...])}
```

**`add_expert_rule(feature, relation, threshold, sign, weight=1.0)`**

Add expert knowledge rule.

```python
model.add_expert_rule(
    feature='age',  # Feature name (str) or index (int)
    relation='>',  # '>', '>=', '<', '<=', '=='
    threshold='mean',  # Numeric value or 'mean', 'median'
    sign=1,  # Expected effect: 1 or -1
    weight=1.0  # Rule importance weight
)
```

**`save(filepath)`** / **`load(filepath)`**

Save/load model.

```python
model.save('texgisa_model.pkl')
loaded_model = TexGISa.load('texgisa_model.pkl')
```

### Metrics Module

```python
from texgisa_survival.metrics import (
    concordance_index,
    brier_score,
    integrated_brier_score
)

# C-index (concordance index)
c_index = concordance_index(time_true, risk_pred, event)

# Brier score at specific time
bs = brier_score(time_true, survival_pred, event, time_point=365)

# Integrated Brier Score
ibs = integrated_brier_score(time_true, survival_pred, event, times=[30, 60, 90])
```

## Performance

### Benchmark Results

Performance on standard survival analysis datasets (C-index, higher is better):

| Dataset  | TexGISa | Cox PH | Random Survival Forest |
|----------|---------|--------|------------------------|
| METABRIC | **0.682** | 0.665  | 0.671                  |
| SUPPORT  | **0.619** | 0.605  | 0.610                  |
| FLCHAIN  | **0.791** | 0.778  | 0.783                  |
| GBSG     | **0.672** | 0.658  | 0.665                  |

*Results averaged over 5-fold cross-validation*

### Computational Efficiency

- **Training time**: ~2-5 minutes for 10,000 samples on GPU
- **Inference**: <1ms per sample on GPU
- **TEXGI computation**: ~100ms for 100 samples
- **Memory**: ~500MB for typical dataset

### Advantages Over Baselines

| Metric | TexGISa | DeepSurv | Cox PH | RSF |
|--------|---------|----------|--------|-----|
| Interpretability | âœ… TEXGI | âŒ | âœ… Coefficients | âŒ |
| Expert Knowledge | âœ… | âŒ | âŒ | âŒ |
| Time-dependent Importance | âœ… | âŒ | âŒ | âŒ |
| Non-linear Modeling | âœ… | âœ… | âŒ | âœ… |
| GPU Acceleration | âœ… | âœ… | âŒ | âŒ |

## Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Setup

```bash
# Clone repository
git clone https://github.com/dhai-lab/texgisa_survival.git
cd texgisa_survival

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install in development mode
pip install -e ".[dev]"

# Run tests
pytest tests/

# Run linting
flake8 src/
black src/ --check
```

### Reporting Issues

Please use the [GitHub issue tracker](https://github.com/dhai-lab/texgisa_survival/issues) to report bugs or request features.

## Citation

If you use TexGISa in your research, please cite both the software and the paper:

### Software Citation

```bibtex
@software{texgisa2024,
  author = {DHAI Lab},
  title = {TexGISa: Time-dependent EXtreme Gradient Integration for Survival Analysis},
  year = {2024},
  url = {https://github.com/dhai-lab/texgisa_survival},
  version = {1.0.0}
}
```

### Paper Citation

```bibtex
@inproceedings{texgisa2024,
  title={TexGISa: Interpretable and Interactive Deep Survival Analysis with Time-dependent Extreme Gradient Integration},
  author={Author Names},
  booktitle={Proceedings of the IEEE International Conference on Data Mining (ICDM)},
  year={2024}
}
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2024 DHAI Lab

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
```

## Acknowledgments

This work builds upon research in interpretable machine learning and survival analysis. We thank:
- The PyTorch team for the deep learning framework
- The scikit-learn team for API design inspiration
- The survival analysis community for datasets and benchmarks

## Related Projects

- [**lifelines**](https://github.com/CamDavidsonPilon/lifelines): Classical survival analysis in Python
- [**scikit-survival**](https://github.com/sebp/scikit-survival): Survival analysis with scikit-learn API
- [**pycox**](https://github.com/havakv/pycox): Deep learning for survival analysis

## Support

- **Documentation**: Coming soon at https://texgisa-survival.readthedocs.io
- **Tutorials**: See `examples/` directory
- **Issues**: [GitHub Issues](https://github.com/dhai-lab/texgisa_survival/issues)
- **Discussions**: [GitHub Discussions](https://github.com/dhai-lab/texgisa_survival/discussions)

---

**Note**: This software implements the TexGISa method published at ICDM 2024. The software is submitted to JMLR Machine Learning Open Source Software (MLOSS). Prior publication of the method is allowed under MLOSS policy, but this specific software implementation has not been published elsewhere.

## Frequently Asked Questions

**Q: How does TexGISa differ from Cox proportional hazards?**
A: TexGISa uses deep neural networks for flexible non-linear modeling and provides time-dependent feature importance through TEXGI, whereas Cox PH assumes linear effects and proportional hazards.

**Q: What is TEXGI and how does it work?**
A: TEXGI (Time-dependent EXtreme Gradient Integration) uses Expected Gradients to compute feature attributions. It creates interpolated paths between inputs and reference baselines, computes gradients along these paths, and integrates them to obtain feature importance scores.

**Q: Can I use expert knowledge even if I'm not sure about the rules?**
A: Yes! The `weight` parameter allows you to specify confidence in each rule. Use lower weights for uncertain rules and higher weights for well-established domain knowledge.

**Q: How many time bins should I use?**
A: Typically 10-20 bins work well. More bins = finer temporal resolution but longer training. Start with 10-15 and adjust based on your data's time range.

**Q: Does TexGISa handle censored data?**
A: Yes! TexGISa fully supports right-censored survival data, which is standard in survival analysis.

**Q: Can I use TexGISa with image or text data?**
A: Yes! You can use any feature representation. For images, extract features using a CNN first. For text, use embeddings. TexGISa works with any tabular feature representation.

**Q: How do I choose the `lambda_expert` parameter?**
A: Start with 0.1. If expert rules are strongly violated, increase to 0.2-0.5. If the model underfits, decrease to 0.05. Use validation data to tune.

**Q: Is GPU required?**
A: No, but recommended for datasets >5000 samples. CPU works fine for smaller datasets.
